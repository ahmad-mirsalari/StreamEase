{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv-TasNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from onnx import shape_inference\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "You need to clone the Conv-TasNet repo here and rename the folder to network:\n",
    "`git@github.com:naplab/Conv-TasNet.git`\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define HyperParameters of Conv-TasNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L =32\n",
    "stride = L // 2\n",
    "\n",
    "num_spk = 1 # number of speakers , Currently only 1 is supported in the streaming mode\n",
    "casual = True # Due to the gLN layer(Therefore, it uses the statistics from all sequence from past to future), the non-causal model cannot run in the streaming mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Conv-TasNet with random weights and input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network.conv_tasnet import TasNet\n",
    "\n",
    "nnet = TasNet(enc_dim=128, feature_dim=64, layer=2, stack=3, \n",
    "                kernel=3, num_spk=num_spk, causal=casual) # Currently supports only 1 speaker\n",
    "\n",
    "receptive_field = nnet.receptive_field\n",
    "\n",
    "onnx_filename = \"convtasnet_orig.onnx\"\n",
    "current_directory = os.getcwd()\n",
    "onnx_path = os.path.join(current_directory, \"\", onnx_filename)\n",
    "\n",
    "x = torch.rand(1, 32000) # a dummy input\n",
    "torch.onnx.export(nnet, x, onnx_path, input_names=[\"input\"], output_names=[\"output\"])\n",
    "print(f\" Model exported to {onnx_path}\")\n",
    "\n",
    "T = int((receptive_field - L) / (L // 2)) + 1 # Number of timestamps of the receptive field\n",
    "\n",
    "print(f\"receptive field is { nnet.receptive_field} samples and {T} timesteps \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the non-streaming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_model_path = \"convtasnet_orig.onnx\"\n",
    "ort_sess = ort.SessionOptions()\n",
    "ort_sess.enable_profiling = True\n",
    "ort_sess  = ort.InferenceSession(ns_model_path)\n",
    "ort_sess.enable_profiling = True\n",
    "\n",
    "# input_data = np.random.rand(1, 32000).astype(np.float32)\n",
    "input_data = x.numpy().astype(np.float32)\n",
    "\n",
    "onnx_model = onnx.load(ns_model_path)\n",
    "# Run the model\n",
    "input_name = onnx_model.graph.input[0].name\n",
    "final_output_model = ort_sess.run(None, {input_name: input_data})\n",
    "\n",
    "# 'output' contains the model's output (replace 'output_name' with the actual output name in your model)\n",
    "print(\"Model output:\", final_output_model[0].shape)\n",
    "print(final_output_model[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the non-streaming model to streaming one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamease.onnx_streamer.streamer import StreamingConverter\n",
    "\n",
    "streaming_name = \"torch_streaming_model.onnx\"\n",
    "time_steps = 1\n",
    "model = onnx.load(ns_model_path)\n",
    "streaming =  StreamingConverter(model, time_steps=time_steps)\n",
    "\n",
    "streaming.run()\n",
    "streaming.print_info()\n",
    "streaming.save_streaming_onnx('.', onnx_filename=streaming_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we need to prepare the correct input for the streaming. For instance, in Conv-TasNet, the following padding function is used before the first layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "def pad_signal( input, stride, win):\n",
    "\n",
    "    # input is the waveforms: (B, T) or (B, 1, T)\n",
    "    # reshape and padding\n",
    "    if input.dim() not in [2, 3]:\n",
    "        raise RuntimeError(\"Input can only be 2 or 3 dimensional.\")\n",
    "    \n",
    "    if input.dim() == 2:\n",
    "        input = input.unsqueeze(1)\n",
    "    batch_size = input.size(0)\n",
    "    nsample = input.size(2)\n",
    "    rest = win - (stride + nsample % win) % win\n",
    "    if rest > 0:\n",
    "        pad = Variable(torch.zeros(batch_size, 1, rest)).type(input.type())\n",
    "        input = torch.cat([input, pad], 2)\n",
    "    \n",
    "    pad_aux = Variable(torch.zeros(batch_size, 1, stride)).type(input.type())\n",
    "    input = torch.cat([pad_aux, input, pad_aux], 2)\n",
    "    return input, rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.onnx_inference.streaming_inference import Inference\n",
    "\n",
    "s_test = Inference(streaming_name, receptive_field=T, causal=casual, time_steps=1)\n",
    "s_test.init_buffers()\n",
    "\n",
    "\n",
    "input_data_pad, rest = pad_signal(torch.tensor(input_data), stride, L)\n",
    "\n",
    "input_data_re = np.reshape(input_data_pad, (input_data_pad.shape[-1]))\n",
    "# input_data_re = np.pad(input_data_re, (receptive_field, 0), 'constant', constant_values=(0, 0))\n",
    "str_output = s_test.run_audio(input_data_re, frame_length= L, stride= stride, transpose=False, dim=2 )\n",
    "out1 = str_output[0][stride:-(rest + stride)]\n",
    "print(out1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the cLN code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "You might notice some differences in the outputs, particularly for timesteps greater than 1. This is because the training model uses cLN on the entire dataset (not just the receptive field). If you set the receptive field in the following code to match that of the training model, the results will be identical.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.onnx_inference.streaming_inference import Inference\n",
    "\n",
    "input_data_pad, rest = pad_signal(torch.tensor(input_data), stride, L)\n",
    "T = (input_data_pad.shape[-1] - L) // stride + 1 \n",
    "print(f\"receptive field is { nnet.receptive_field} samples and {T} timesteps \")\n",
    "\n",
    "input_data_re = np.reshape(input_data_pad, (input_data_pad.shape[-1]))\n",
    "# input_data_re = np.pad(input_data_re, (receptive_field, 0), 'constant', constant_values=(0, 0))\n",
    "s_test = Inference(streaming_name, receptive_field=T, causal=casual, time_steps=1)\n",
    "s_test.init_buffers()\n",
    "str_output = s_test.run_audio(input_data_re, frame_length= L, stride= stride, transpose=False, dim=2 )\n",
    "out1 = str_output[0][stride:-(rest + stride)]\n",
    "print(out1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NNTOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nntool.api import NNGraph\n",
    "from nntool.api.utils import model_settings, quantization_options, tensor_plot\n",
    "import logging\n",
    "# nntool_log = logging.getLogger('nntool')\n",
    "# nntool_log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_model = \"torch_streaming_model.onnx\" \n",
    "s_model = NNGraph.load_graph(streaming_model, use_onnx_names=True)\n",
    "# s_model.draw()\n",
    "s_model.adjust_order()\n",
    "# s_model.draw()\n",
    "# The equivalent of the fusions --scale8 command. The fusions method can be given a series of fusions to apply\n",
    "# fusions('name1', 'name2', etc)\n",
    "s_model.fusions('scaled_match_group')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nntool_inference.streaming_inference import Inference\n",
    "print(f\"Receptive field is {T}\")\n",
    "s_test = Inference(s_model, streaming_model, time_steps=1, receptive_field=T, causal=True)\n",
    "\n",
    "s_test.init_buffers()\n",
    "input_data_pad, rest = pad_signal(torch.tensor(input_data), stride, L)\n",
    "input_data_re = np.reshape(input_data_pad, (input_data_pad.shape[-1]))\n",
    "# input_data = np.reshape(input_data, (1,32,28))\n",
    "\n",
    "# input_tra = np.transpose(input_data, (0,2,1))\n",
    "print(input_data_re.shape)\n",
    "nn_str_output = s_test.run_audio(input_data_re, frame_length= L, stride= stride, transpose=True, dim=2 )\n",
    "print(nn_str_output[0][stride:-(rest + stride)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwt_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
