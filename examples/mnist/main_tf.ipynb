{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: MNIST dataset in tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from onnx import shape_inference\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import streamease as se"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data_loader import load_mnist_data\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (31, 32)\n",
    "\n",
    "# Call the functions with the instance\n",
    "train_data, test_data,noisy_train_data, noisy_test_data = load_mnist_data(row=input_shape[0], column=input_shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple TCN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "def Net(input_shape, padding='causal'):\n",
    "    x_input = Input(shape=input_shape, name='input')\n",
    "\n",
    "    # TCN layers\n",
    "    x = Conv1D(filters=36, kernel_size=3, dilation_rate=1, padding=padding, activation='relu')(x_input)\n",
    "    x = Conv1D(filters=40, kernel_size=5, dilation_rate=2, padding=padding, activation='relu')(x)\n",
    "    out = Conv1D(filters=input_shape[1], kernel_size=2, dilation_rate=4, padding=padding)(x)\n",
    "\n",
    "    # out = Dense(28, activation='sigmoid')(x)  # Output layer\n",
    "\n",
    "    model = Model(inputs=[x_input], outputs=[out])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.002), loss='mean_squared_error')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Train and Test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train , y_train, x_validate, y_validate, epochs, batch_size):\n",
    "        model.fit(\n",
    "                x=x_train,\n",
    "                y=y_train,\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_validate, y_validate),\n",
    "        )\n",
    "        return model\n",
    "def test(model, x_validate, test_size):\n",
    "        predictions = model.predict(x_validate[:test_size])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run and test the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_shape, padding='causal')\n",
    "model.summary()\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "model = train(model, noisy_train_data,train_data, noisy_test_data, test_data,epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del noisy_train_data\n",
    "del train_data\n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1\n",
    "predictions  = test(model, noisy_test_data, test_size)\n",
    "print(f\"prediction{predictions}\")\n",
    "\n",
    "from helpers.preprocessing import display\n",
    "display(noisy_test_data[:test_size], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import os\n",
    "from tensorflow import keras\n",
    "\n",
    "def save_onnx(model, path='.', onnx_filename='original_model'):\n",
    "    # Construct the full path to save the ONNX model\n",
    "    full_path = os.path.join(path, f\"{onnx_filename}.onnx\")\n",
    "\n",
    "    # Assuming `model` is your Keras model\n",
    "    # Retrieve the input tensor from the first layer\n",
    "    input_tensor = model.layers[0]._input_tensor\n",
    "    input_signature = tf.TensorSpec(\n",
    "        name=input_tensor.name, shape=input_tensor.shape, dtype=input_tensor.dtype\n",
    "    )\n",
    "    \n",
    "    # Get the output name from the last layer\n",
    "    output_name = model.layers[-1].name\n",
    "\n",
    "    # Wrap the model in a tf.function with the input signature\n",
    "    @tf.function(input_signature=[input_signature])\n",
    "    def _wrapped_model(input_data):\n",
    "        return {output_name: model(input_data)}\n",
    "\n",
    "    # Convert the wrapped model to ONNX\n",
    "    tf2onnx.convert.from_function(\n",
    "        function=_wrapped_model,\n",
    "        input_signature=[input_signature],\n",
    "        output_path=full_path\n",
    "    )\n",
    "\n",
    "    print(f\"ONNX model saved to {full_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_onnx(model, path='.', onnx_filename='original_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert it to streaming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"original_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamease.onnx_streamer.streamer import StreamingConverter\n",
    "\n",
    "streaming =  StreamingConverter(onnx_model, 1)\n",
    "\n",
    "\n",
    "streaming.run()\n",
    "\n",
    "streaming.print_info()\n",
    "streaming.save_streaming_onnx('.', onnx_filename='streaming_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the modified ONNX model\n",
    "modified_model = onnx.load(\"streaming_model.onnx\")\n",
    "\n",
    "# Infer shapes\n",
    "inferred_model = shape_inference.infer_shapes(modified_model)\n",
    "\n",
    "# Save the inferred model (optional)\n",
    "onnx.save(inferred_model, \"inferred_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference using streaming onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the non-streaming onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_model = onnx.load(\"original_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"original_model.onnx\"\n",
    "ort_sess  = ort.InferenceSession(model_path)\n",
    "\n",
    "\n",
    "input_data = noisy_test_data[:1].astype(np.float32)\n",
    "# print(f\"input data {noisy_test_data[:test_size]}\")\n",
    "\n",
    "# Run the model\n",
    "output = ort_sess .run(None, {'input': input_data})\n",
    "\n",
    "# 'output' contains the model's output (replace 'output_name' with the actual output name in your model)\n",
    "print(\"Model output:\", output[0].shape)\n",
    "display(input_data, output[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.onnx_inference.streaming_inference import Inference\n",
    "\n",
    "original_model =\"original_model.onnx\"\n",
    "streaming_model = \"streaming_model.onnx\" \n",
    "s_test = Inference(streaming_model,time_steps=1, receptive_field=14, causal=True)\n",
    "\n",
    "s_test.init_buffers()\n",
    "\n",
    "str_output = s_test.run(input_data)\n",
    "display(input_data[:1],str_output[:1])\n",
    "print(str_output[:1].shape)\n",
    "print(str_output[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to check the non-causal network, we need to create a non-causal model and then transfer the weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name)\n",
    "    for weight in layer.weights:\n",
    "        print(weight.name, weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nc_model = Net(input_shape, padding=\"valid\")\n",
    "nc_model.set_weights(model.get_weights())\n",
    "nc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_onnx(nc_model, path='.', onnx_filename='nc_original_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the ONNX model\n",
    "onnx_model = onnx.load(\"nc_original_model.onnx\")\n",
    "\n",
    "streaming =  StreamingConverter(onnx_model, 1)\n",
    "\n",
    "\n",
    "streaming.run()\n",
    "\n",
    "streaming.print_info()\n",
    "streaming.save_streaming_onnx('.', onnx_filename='streaming_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "onnx_model = onnx.load(\"nc_original_model.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# Load the ONNX model\n",
    "model_path = \"nc_original_model.onnx\"\n",
    "ort_sess  = ort.InferenceSession(model_path)\n",
    "\n",
    "\n",
    "input_data = noisy_test_data[:1].astype(np.float32)\n",
    "# print(f\"input data {noisy_test_data[:test_size]}\")\n",
    "\n",
    "# Run the model\n",
    "output = ort_sess .run(None, {'input': input_data})\n",
    "\n",
    "# 'output' contains the model's output (replace 'output_name' with the actual output name in your model)\n",
    "print(\"Model output:\", output[0].shape)\n",
    "display(input_data, output[0])\n",
    "print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model =\"nc_original_model.onnx\"\n",
    "streaming_model = \"streaming_model.onnx\" \n",
    "s_test = Inference(streaming_model,time_steps=1, receptive_field=14, causal=False)\n",
    "\n",
    "s_test.init_buffers()\n",
    "\n",
    "str_output = s_test.run(input_data)\n",
    "display(input_data[:1],str_output[:1])\n",
    "print(str_output[:1].shape)\n",
    "print(str_output[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNTOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nntool.api import NNGraph\n",
    "from nntool.api.utils import model_settings, quantization_options, tensor_plot\n",
    "# import logging\n",
    "# nntool_log = logging.getLogger('nntool')\n",
    "# nntool_log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNGraph.load_graph('original_model.onnx', use_onnx_names=True)\n",
    "\n",
    "model.adjust_order()\n",
    "model.fusions('scaled_match_group')\n",
    "    \n",
    "# Model show returns a table of information on the Graph\n",
    "print(model.show())\n",
    "\n",
    "# Model draw can open or save a PDF with a visual representation of the graph\n",
    "# model.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = noisy_test_data[:1].astype(np.float32)\n",
    "print(data.shape)\n",
    "output=model.execute(np.transpose(data, (0, 2, 1)))[-1] #[0]\n",
    "\n",
    "for item in output:\n",
    "    item = np.transpose(item, (0, 2, 1))\n",
    "    print(item)\n",
    "    display(data, item[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nntool_inference.streaming_inference import Inference\n",
    "\n",
    "# Set printing options to display larger numbers without scientific notation\n",
    "np.set_printoptions(precision=8, suppress=False, threshold=np.inf)\n",
    "\n",
    "streaming_model_path = \"streaming_model.onnx\" \n",
    "s_model = NNGraph.load_graph(streaming_model_path, use_onnx_names=True)\n",
    "s_model.adjust_order()\n",
    "# s_model.draw()\n",
    "# The equivalent of the fusions --scale8 command. The fusions method can be given a series of fusions to apply\n",
    "# fusions('name1', 'name2', etc)\n",
    "s_model.fusions('scaled_match_group')\n",
    "\n",
    "s_test = Inference( s_model, streaming_model_path, receptive_field=14, time_steps=1, causal=True)\n",
    "\n",
    "s_test.init_buffers()\n",
    "input_data = noisy_test_data[:1].astype(np.float32)\n",
    "# input_data = np.reshape(input_data, (1,32,28))\n",
    "\n",
    "# input_data = np.transpose(input_data, (0,2,1))\n",
    "print(input_data[:1].shape)\n",
    "nn_str_output = s_test.run(input_data, i_transpose=True, o_transpose=False)\n",
    "display(input_data[:1],nn_str_output[:1])\n",
    "for i in range(len(nn_str_output)):\n",
    "    print(\"#######################################\")\n",
    "    # print(\"golden model\", output[i])\n",
    "    print(\"streaming model\", nn_str_output[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwt_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
