{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: build a network using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "from onnx import shape_inference\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import streamease as se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.data_loader import load_mnist_data\n",
    "\n",
    "# Example usage:\n",
    "input_shape = (28, 32)\n",
    "\n",
    "# Call the functions with the instance\n",
    "train_data, test_data,noisy_train_data, noisy_test_data = load_mnist_data(input_shape[0], input_shape[1], transpose= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def CausalConv1d(in_channels, out_channels, kernel_size, dilation=1, **kwargs):\n",
    "   pad = (kernel_size - 1) * dilation\n",
    "   return nn.Conv1d(in_channels, out_channels, kernel_size, padding=pad, dilation=dilation, bias=False, **kwargs)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Create the model layers\n",
    "        self.conv1 = CausalConv1d(in_channels=input_size, out_channels=36, kernel_size=3, dilation=1)\n",
    "        self.conv2 = CausalConv1d(in_channels=36, out_channels=40, kernel_size=5, dilation=2)\n",
    "        self.conv3 = CausalConv1d(in_channels=40, out_channels=input_size, kernel_size=2, dilation=4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        if self.conv1.padding[0] != 0:\n",
    "            x = x[:, :, :-self.conv1.padding[0]]  # remove trailing padding\n",
    "\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        if self.conv2.padding[0] != 0:\n",
    "            x = x[:, :, :-self.conv2.padding[0]]  # remove trailing padding\n",
    "\n",
    "        out = self.conv3(x)\n",
    "        if self.conv3.padding[0] != 0:\n",
    "            out = out[:, :, :-self.conv3.padding[0]]  # remove trailing padding\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, num_epochs=4):\n",
    "    # Define the loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.002)  \n",
    "    for epoch in range(num_epochs):\n",
    "        for data in train_loader:\n",
    "            inputs = data[0].float()  # Assuming float data type\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    return model\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs = data[0].float()\n",
    "            outputs = model(inputs)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "    return torch.cat(all_outputs, dim=0)\n",
    "def save_onnx(model, dummy_input, path='.', onnx_filename='torch_original_model'):\n",
    "    full_path = os.path.join(path, f\"{onnx_filename}.onnx\")\n",
    "    # Export the model to ONNX\n",
    "    torch.onnx.export(model, (dummy_input,), full_path, verbose=False, input_names=['input'], output_names=['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of the class\n",
    "torch_model = Net(input_shape[1]) \n",
    "\n",
    "print(torch_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing import torch_data_loader\n",
    "batch_size =128\n",
    "train_loader = torch_data_loader( noisy_train_data, batch_size)\n",
    "test_loader = torch_data_loader(noisy_test_data, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "\n",
    "torch_model = train(torch_model, train_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = test(torch_model, test_loader)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del noisy_train_data\n",
    "del train_data\n",
    "del test_data\n",
    "del train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the resutls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.preprocessing import display\n",
    "# Display the original and reconstructed images\n",
    "torch_outputs =  np.transpose (outputs.cpu().numpy(), (0,2,1))\n",
    "\n",
    "# Assuming you have a DataLoader named train_loader\n",
    "data_iter = iter(test_loader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "# Now first_batch contains the first batch of data\n",
    "inputs = first_batch[0].float()  # Assuming float data type\n",
    "\n",
    "inputs_tr = np.transpose(inputs, (0,2,1))\n",
    "display(inputs_tr[:1], torch_outputs[:1])\n",
    "print(torch_outputs[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save the onnx file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_onnx(torch_model, inputs[:1], '.', 'torch_original_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert it to streaming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "# Load the ONNX model\n",
    "model = onnx.load(\"torch_original_model.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamease.onnx_streamer.streamer import StreamingConverter\n",
    "streaming =  StreamingConverter(model, time_steps=1)\n",
    "\n",
    "streaming.run()\n",
    "\n",
    "streaming.print_info()\n",
    "streaming.save_streaming_onnx('.', onnx_filename='torch_streaming_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the streaming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.onnx_inference.streaming_inference import Inference\n",
    "\n",
    "original_model =\"torch_original_model.onnx\"\n",
    "streaming_model = \"torch_streaming_model.onnx\" \n",
    "s_test = Inference( streaming_model, time_steps=1, receptive_field=14, causal=True)\n",
    "\n",
    "s_test.init_buffers()\n",
    "input_data = inputs[:1].cpu().numpy()\n",
    "# input_data = np.reshape(input_data, (1,32,28))\n",
    "\n",
    "input_data = np.transpose(input_data, (0,2,1))\n",
    "print(input_data.shape)\n",
    "str_output = s_test.run(input_data, transpose=True)\n",
    "display(input_data[:1],str_output[:1])\n",
    "\n",
    "print(str_output[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNTOOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nntool.api import NNGraph\n",
    "from nntool.api.utils import model_settings, quantization_options, tensor_plot\n",
    "import logging\n",
    "# nntool_log = logging.getLogger('nntool')\n",
    "# nntool_log.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNGraph.load_graph('torch_original_model.onnx', use_onnx_names=True)\n",
    "model.adjust_order()\n",
    "model.fusions('scaled_match_group')\n",
    "    \n",
    "# Model show returns a table of information on the Graph\n",
    "print(model.show())\n",
    "\n",
    "# Model draw can open or save a PDF with a visual representation of the graph\n",
    "# model.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the non-streaming model in nntool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inputs[:1].cpu().numpy()\n",
    "output=model.execute(data)[-1]\n",
    "print(output)\n",
    "\n",
    "for item in output:\n",
    "    display(data, item[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the streaming model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.nntool_inference.streaming_inference import Inference\n",
    "\n",
    "original_model =\"torch_original_model.onnx\"\n",
    "streaming_model = \"torch_streaming_model.onnx\" \n",
    "s_model = NNGraph.load_graph(streaming_model, use_onnx_names=True)\n",
    "# s_model.draw()\n",
    "s_model.adjust_order()\n",
    "# s_model.draw()\n",
    "# The equivalent of the fusions --scale8 command. The fusions method can be given a series of fusions to apply\n",
    "# fusions('name1', 'name2', etc)\n",
    "s_model.fusions('scaled_match_group')\n",
    "\n",
    "s_test = Inference(s_model, streaming_model, time_steps=1, receptive_field=14, causal=True)\n",
    "\n",
    "s_test.init_buffers()\n",
    "input_data = inputs[:1].cpu().numpy()\n",
    "# input_data = np.reshape(input_data, (1,32,28))\n",
    "\n",
    "input_tra = np.transpose(input_data, (0,2,1))\n",
    "print(input_data[:1].shape)\n",
    "nn_str_output = s_test.run(input_tra, i_transpose=False, o_transpose=False)\n",
    "f_output = np.transpose(nn_str_output, (0,2,1))\n",
    "display(input_data[:1],f_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(nn_str_output)):\n",
    "    print(\"#######################################\")\n",
    "    # print(\"golden model\", torch_outputs[i])\n",
    "    print(\"streaming model\", nn_str_output[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gwt_tools",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
